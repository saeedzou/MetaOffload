{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/saeedzou/MetaOffload &> /dev/null\n",
    "%cd /content/MetaOffload\n",
    "!pip install -q gym==0.14.0 &> /dev/null\n",
    "!pip install wandb -qU &> /dev/null\n",
    "!pip install -q pydotplus &> dev\\null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.distributions import Categorical\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from env.mec_offloaing_envs.offloading_env import Resources\n",
    "from env.mec_offloaing_envs.offloading_env import OffloadingEnvironment\n",
    "from models import GraphSeq2Seq, BaselineSeq2Seq\n",
    "from buffer import RolloutBuffer\n",
    "from train import inner_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('baseline_config.json') as f:\n",
    "    args = json.load(f)\n",
    "\n",
    "class Config:\n",
    "        def __init__(self, dictionary):\n",
    "            for key, value in dictionary.items():\n",
    "                setattr(self, key, value)\n",
    "\n",
    "c = Config(args)\n",
    "\n",
    "np.random.seed(c.seed)\n",
    "torch.manual_seed(c.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resources = Resources(mec_process_capable=c.mec_process_capable*1024*1024,\n",
    "                      mobile_process_capable=c.mobile_process_capable*1024*1024,\n",
    "                      bandwidth_up=c.bandwidth_up,\n",
    "                      bandwidth_dl=c.bandwidth_down)\n",
    "\n",
    "env = OffloadingEnvironment(resource_cluster=resources,\n",
    "                            batch_size=c.graph_number,\n",
    "                            graph_number=c.graph_number,\n",
    "                            graph_file_paths=[\"./env/mec_offloaing_envs/data/meta_offloading_20/offload_random20_12/random.20.\"],\n",
    "                            time_major=False,\n",
    "                            encoding=c.encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if c.wandb:\n",
    "    import wandb\n",
    "    wandb.login(key=c.wandb_key)\n",
    "    wandb.init(project=c.wandb_project,\n",
    "               name=c.wandb_name,\n",
    "               config=c)\n",
    "\n",
    "print(f'Average greedy latency: {np.mean(env.greedy_solution()[1]):.4f}')\n",
    "print(f'Average all local latency: {np.mean(env.get_all_locally_execute_time()):.4f}')\n",
    "print(f'Average all mec latency: {np.mean(env.get_all_mec_execute_time()):.4f}')\n",
    "\n",
    "device = c.device\n",
    "c.meta_batch_size = 1\n",
    "latencies = []\n",
    "\n",
    "if c.is_graph:\n",
    "    policy = GraphSeq2Seq(input_dim=c.obs_dim,\n",
    "                          hidden_dim=c.encoder_units,\n",
    "                          output_dim=c.action_dim,\n",
    "                          num_layers=c.num_layers,\n",
    "                          device=device).to(device)\n",
    "else:\n",
    "    policy = BaselineSeq2Seq(input_dim=c.obs_dim,\n",
    "                             hidden_dim=c.encoder_units,\n",
    "                             output_dim=c.action_dim,\n",
    "                             num_layers=c.num_layers,\n",
    "                             device=device).to(device)\n",
    "if args[\"load\"]:\n",
    "    policy.load_state_dict(torch.load(args[\"load_path\"]))\n",
    "\n",
    "buffer = RolloutBuffer(meta_batch_size=c.meta_batch_size, \n",
    "                       buffer_size=c.graph_number*c.num_task_episodes, \n",
    "                       discount=c.gamma, \n",
    "                       gae_lambda=c.tau, \n",
    "                       device=device)\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=c.inner_lr)\n",
    "\n",
    "for iteration in tqdm(range(c.start_iter, c.num_iterations), leave=False, disable=True):\n",
    "    task_policies = []\n",
    "    fts_before, fts_after = [], []\n",
    "    vf_losses, pg_losses = [], []\n",
    "    all_rewards, all_returns = [], []\n",
    "    \n",
    "    batch_of_tasks = env.sample_tasks(c.meta_batch_size)\n",
    "\n",
    "\n",
    "    ### Sample trajectories ###\n",
    "    buffer.reset()\n",
    "    for i, task_id in tqdm(enumerate(batch_of_tasks), leave=False, total=c.meta_batch_size, desc=f'Sampling trajectories'):\n",
    "        buffer.collect_episodes(env=env, \n",
    "                                policy=policy, \n",
    "                                device=device, \n",
    "                                meta_batch=i, \n",
    "                                task_id=task_id, \n",
    "                                is_graph=c.is_graph)\n",
    "    buffer.process()\n",
    "    \n",
    "    for i, task_id in enumerate(batch_of_tasks):\n",
    "        vf_loss, pg_loss, fts, policy = \\\n",
    "            inner_loop(policy=policy, \n",
    "                       optimizer=optimizer, \n",
    "                       buffer=buffer, \n",
    "                       meta_batch=i, \n",
    "                       task_id=task_id, \n",
    "                       hparams=c)\n",
    "        vf_losses.append(vf_loss)\n",
    "        pg_losses.append(pg_loss)\n",
    "        fts_before.append(fts)\n",
    "        task_policies.append(policy)\n",
    "\n",
    "    print('*'*50)\n",
    "    latencies.append(np.mean(np.concatenate(fts_before)))\n",
    "    print(\"Iteration\", iteration,\n",
    "        \"| vf_loss: {:.4f}\".format(np.mean(vf_losses)),\n",
    "        \"| pg_loss: {:.4f}\".format(np.mean(pg_losses)),\n",
    "        \"| average_reward: {:.4f}\".format(np.mean([reward.sum(-1) for reward in buffer.rewards])),\n",
    "        \"| average_return: {:.4f}\".format(np.mean([returns[:, 0].mean().item() for returns in buffer.returns])),\n",
    "        \"| latency before adaptation: {:.4f}\".format(np.mean(np.concatenate(fts_before))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tez",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
